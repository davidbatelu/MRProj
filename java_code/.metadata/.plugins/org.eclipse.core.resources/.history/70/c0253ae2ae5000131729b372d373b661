package org.myorg;

import java.io.IOException;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.*;
import org.apache.hadoop.mapreduce.*;
import org.apache.hadoop.mapreduce.Mapper.Context;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.input.TextInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
import org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;
import org.myorg.Collab.PreMap;
import org.myorg.Collab.PreReduce;

//Trying to use HashMaps for now
//import no.uib.cipr.matrix.sparse.SparseVector;


public class BuildSongsMat {
	public static class CollabMap extends Mapper<LongWritable, Text, Text, Text> {
		public void map(LongWritable key, Text value, Context context) throws InterruptedException, IOException {
			String string = value.toString();
//			parts = (user_id, song_id, play_count)
			String[] parts = string.split("\t");
//			Not sending the counts ahead as we are binning the vector
			context.write(new Text(parts[0]), new Text(parts[1]));
		}
	}
	
		
	public static class PreReduce extends Reducer<Text, IntWritable, Text, IntWritable> {
	   public void reduce(Text key, Iterable<IntWritable> values, Context context) 
	     throws IOException, InterruptedException {
	       int sum = 0;
	       for (IntWritable val : values) {
	           sum += val.get();
	       }
	       context.write(key, new IntWritable(sum));
	   }
	}
	
	public static void main(String[] args) throws Exception {
	 Configuration conf = new Configuration();
	 long unixTime = System.currentTimeMillis() / 1000L;

	 Job job = new Job(conf, "collab");
	 job.setOutputKeyClass(Text.class);
	 job.setOutputValueClass(IntWritable.class);
	 job.setMapperClass(PreMap.class);
	 job.setReducerClass(PreReduce.class);
	 job.setMapOutputKeyClass(Text.class);
	 job.setMapOutputValueClass(Text.class);
	 job.setInputFormatClass(TextInputFormat.class);
	 job.setOutputFormatClass(TextOutputFormat.class);
//				 job.setCombinerClass(Reduce.class);
//				 job.setPartitionerClass(WordPartitioner.class);
//				 job.setNumReduceTasks(5);
	 
	 job.setJarByClass(Collab.class);

//			     FileInputFormat.addInputPath(job, new Path(args[0]));
//			     FileOutputFormat.setOutputPath(job, new Path(args[1]));
     FileInputFormat.addInputPath(job, new Path("/Users/dave/proj/MRProj/data/train_triplets.txt"));
     FileOutputFormat.setOutputPath(job, new Path("/Users/dave/proj/MRProj/output/" + Long.toString(unixTime)));
     job.waitForCompletion(true);
   }
	
}
